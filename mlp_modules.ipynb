{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_prep():\n",
    "    import pandas as pd, sys, re, datetime, time\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.dates as mdates\n",
    "    import seaborn as sns\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    %matplotlib inline\n",
    "    \n",
    "    # Read data\n",
    "    url = \"https://aisgaiap.blob.core.windows.net/aiap5-assessment-data/traffic_data.csv\"\n",
    "    df = pd.read_csv(url, parse_dates = ['date_time'])\n",
    "    df.drop('snow_1h', axis = 1, inplace = True)\n",
    "    \n",
    "    # Fix weather_description\n",
    "    df['weather_description'] = df['weather_description'].str.lower()\n",
    "    \n",
    "    # Get the repeated date_time values\n",
    "    rep_date = pd.DataFrame(df['date_time'].value_counts()).reset_index()\n",
    "    rep_date = pd.DataFrame(rep_date[rep_date['date_time'] > 1]['index'])\n",
    "    rep_date.rename(columns = {'index': 'date_time'}, inplace = True)\n",
    "    \n",
    "    # To ensure concatenation of weather_main strings is done correctly, avoid duplicates\n",
    "    df.sort_values(by = 'weather_main', inplace = True)\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    # Concatenation of weather_main and weather_description\n",
    "    df_rep = pd.merge(df, rep_date, how = 'inner', on = 'date_time')\n",
    "    df_rep_wt_main = pd.DataFrame(df_rep.groupby(df_rep.columns.difference(['weather_main', 'weather_description']).tolist())['weather_main'].apply(lambda x: \"%s\" % ', '.join(x.unique()))).reset_index()\n",
    "    df_rep_wt_desc = pd.DataFrame(df_rep.groupby('date_time')['weather_description'].apply(lambda x: \"%s\" % ', '.join(x.unique()))).reset_index()\n",
    "    df_rep = pd.merge(df_rep_wt_main, df_rep_wt_desc, how = 'inner', on = 'date_time')\n",
    "    \n",
    "    # Remove the repeated rows of date_time\n",
    "    df.drop_duplicates(subset =['date_time', 'traffic_volume'], keep = False, inplace = True)\n",
    "    \n",
    "    # Append the new cleaned rows of date_time\n",
    "    df = df.append(df_rep, sort = True)\n",
    "    \n",
    "    # Extract date features from date_time\n",
    "    def add_datepart(df, date_field, drop=False, time=False, errors=\"raise\"):\n",
    "        fld = df[date_field]\n",
    "        fld_dtype = fld.dtype\n",
    "        if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "            fld_dtype = np.datetime64\n",
    "    \n",
    "        if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "            df[date_field] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n",
    "        targ_pre = re.sub('[Dd]ate$', '', date_field)\n",
    "        attr = ['Month', 'Day', 'Hour', 'Date','Weekday']\n",
    "        if time: \n",
    "            attr = attr + ['Hour', 'Minute', 'Second']\n",
    "        for n in attr: \n",
    "            df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "        if drop: \n",
    "            df.drop(date_field, axis=1, inplace=True)\n",
    "        \n",
    "    add_datepart(df, 'date_time')\n",
    "    \n",
    "    # Fix Date data type\n",
    "    df['date_timeDate'] =  pd.to_datetime(df['date_timeDate'], format='%Y-%m-%d')\n",
    "    \n",
    "    # Clean Holiday feature\n",
    "    df_hols = df[df['holiday'] != 'None'][['holiday', 'date_timeDate']]\n",
    "\n",
    "    df = pd.merge(df, df_hols, how = 'left', on = ['date_timeDate']).drop(columns = 'holiday_x')\n",
    "    df.rename(columns = {'holiday_y': 'holiday'}, inplace = True)\n",
    "    df['holiday'].fillna('None', inplace = True)\n",
    "\n",
    "    df['is_Holiday'] = df['holiday'].apply(lambda x: 1 if x != 'None' else 0)\n",
    "\n",
    "    df_holiday = df[df['is_Holiday'] == 1][['holiday', 'date_timeDate']].drop_duplicates()\n",
    "\n",
    "    post = pd.DataFrame(df_holiday['date_timeDate'] + timedelta(days = 1))\n",
    "    post['prepost_holiday'] = 'Post '+ df_holiday['holiday']\n",
    "    post['is_prepost_hols'] = 2\n",
    "    pre = pd.DataFrame(df_holiday['date_timeDate'] + timedelta(days = -1))\n",
    "    pre['prepost_holiday'] = 'Pre '+ df_holiday['holiday']\n",
    "    pre['is_prepost_hols'] = 1\n",
    "    hols = pre.append(post, ignore_index = True)\n",
    "\n",
    "    df = pd.merge(df, hols, how = 'left', on = ['date_timeDate'])\n",
    "    df['prepost_holiday'].fillna('None', inplace = True)\n",
    "    df['is_prepost_hols'].fillna(0, inplace = True)\n",
    "\n",
    "    # Prepare Categorical Features - to be encoded\n",
    "\n",
    "    df_cat = df[['date_time', 'weather_main', 'holiday']]\n",
    "    df_cat_encoded = pd.get_dummies(df_cat)\n",
    "    df = pd.merge(df, df_cat_encoded, how = 'inner', on = ['date_time'])\n",
    "\n",
    "    # Drop Unecessary Features for final dataframe\n",
    "\n",
    "    df.drop(columns = ['weather_main', 'weather_description', 'holiday', 'prepost_holiday', 'date_time', 'date_timeDate'], inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traffic_model(df):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn import metrics\n",
    "    import math\n",
    "    \n",
    "    # Get independent and dependent variable\n",
    "    X = df.drop('traffic_volume', axis=1)\n",
    "    y = df['traffic_volume']\n",
    "    \n",
    "    # Train-test_validation Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=10)\n",
    "    \n",
    "    # Metrics used to validate: RMSE\n",
    "    def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n",
    "    \n",
    "    # To print scores\n",
    "    def print_score(m):\n",
    "        res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n",
    "                    m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "        if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
    "        print('\\nRoot Mean Squared Error of Predicted and Actual in on Training Set: '+ str(res[0]))\n",
    "        print('Root Mean Squared Error of Predicted and Actual in Validation Set: '+ str(res[1]))\n",
    "        print('R Squared score using Training Set: '+ str(res[2]))\n",
    "        print('R Squared score using Validation Set: '+ str(res[3]))\n",
    "    \n",
    "    # Final model\n",
    "    model = RandomForestRegressor(n_estimators=40, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    return print_score(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
